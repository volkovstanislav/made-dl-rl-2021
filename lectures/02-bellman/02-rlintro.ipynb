{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(\"\\n\".join([\"%s\" % x for x in envs.registry.all()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS, nA = env.env.nS, env.env.nA\n",
    "final_states = np.where([ len(env.env.P[x][0]) == 1 and env.env.P[x][0][0][3] == True for x in env.env.P.keys() ])[0]\n",
    "\n",
    "def get_random_V(env):\n",
    "    V = np.random.random(nS)\n",
    "    V[final_states] = 0.0\n",
    "    return V\n",
    "\n",
    "def get_random_Q(env):\n",
    "    Q = np.random.random(size=(nS, nA))\n",
    "    Q[final_states, :] = 0.0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_by_policy(env, pi, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][pi[cur_state]] ]) \\\n",
    "        for cur_state in range(nS) ])\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_policy_by_V(env, V, gamma=1.0):\n",
    "    return np.argmax( np.array([[ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[s][a] ]) \\\n",
    "        for a in range(nA) ] for s in range(nS)]), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_and_pi(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    pi = np.random.randint(nA, size=nS)\n",
    "\n",
    "    while True:\n",
    "        V = compute_V_by_policy(env, pi, gamma)\n",
    "        new_pi = compute_policy_by_V(env, V, gamma)\n",
    "        if np.array_equal(pi, new_pi):\n",
    "            break\n",
    "        pi = new_pi\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "num_experiments = 200\n",
    "num_steps, total_reward = [], []\n",
    "\n",
    "V, pi = compute_V_and_pi(env)\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    env.reset()\n",
    "    total_reward.append(0)\n",
    "    for step in range(1000):\n",
    "        observation, reward, done, info = env.step(pi[env.env.s])\n",
    "        total_reward[-1] += reward\n",
    "        if done:\n",
    "            num_steps.append(step+1)\n",
    "            print(\"Эпизод закончился за %d шагов в состоянии %s, общая награда %d\" % (num_steps[-1], env.env.s, total_reward[-1]) )\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "print(\"\\nСредняя награда: %.6f\\nСреднее число шагов: %.6f\" % (np.mean(total_reward), np.mean(num_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiments_pi(env, pi, num_experiments=1000):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        env.reset()\n",
    "        num_steps.append(0)\n",
    "        total_reward.append(0)\n",
    "        for _ in range(1000):\n",
    "            observation, reward, done, info = env.step(pi[env.env.s])\n",
    "            total_reward[-1] += reward\n",
    "            num_steps[-1] += 1\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return np.mean(total_reward), np.mean(num_steps)\n",
    "\n",
    "def conduct_experiments(env, gamma=1.0, num_experiments=100, num_experiments_pi=10):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        V, pi = compute_V_and_pi(env, gamma=gamma)\n",
    "        cur_steps, cur_reward = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(cur_steps)\n",
    "        total_reward.append(cur_reward)\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "results = []\n",
    "for gamma in np.linspace(0.5, 1.0, 10):\n",
    "    mean_reward, mean_steps = conduct_experiments(env, gamma, num_experiments=100, num_experiments_pi=10)\n",
    "    results.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    gammas, rewards, numsteps = [x[0] for x in results], [x[1] for x in results], [x[2] for x in results]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.grid(None)\n",
    "\n",
    "    line1 = ax.plot(gammas, rewards, label=\"Средние награды\", color=\"C0\")\n",
    "    line2 = ax2.plot(gammas, numsteps, label=\"Среднее число шагов\", color=\"C1\")\n",
    "\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc=2)\n",
    "    ax.set_xlim((0.5, 1.0))\n",
    "    # ax.set_ylim((0.1, 0.8))\n",
    "    # ax2.set_ylim((10, 45))\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_max(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "        new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_Q_max(env, gamma=1.0):\n",
    "    Q = get_random_Q(env)\n",
    "    while True:\n",
    "        new_Q = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * np.max(Q[x[1], :]) ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "#         new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((Q - new_Q) ** 2) < 0.001:\n",
    "            break\n",
    "        Q = new_Q\n",
    "    return Q\n",
    "\n",
    "def compute_policy_by_Q(env, Q, gamma=1.0):\n",
    "    return np.argmax( Q, axis=1 )\n",
    "\n",
    "def conduct_experiments_max(env, gamma, use_Q=False, num_experiments=100, num_experiments_pi=200):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        if use_Q:\n",
    "            Q = compute_Q_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_Q(env, Q)\n",
    "        else:\n",
    "            V = compute_V_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_V(env, V)\n",
    "        result = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(result[0])\n",
    "        total_reward.append(result[1])\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "V = compute_V_max(env)\n",
    "pi = compute_policy_by_V(env, V, gamma=0.2)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "Q = compute_Q_max(env)\n",
    "pi = compute_policy_by_Q(env, Q, gamma=0.2)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "results_max = []\n",
    "for gamma in np.linspace(0.5, 1.0, 20):\n",
    "    mean_reward, mean_steps = conduct_experiments_max(env, gamma, use_Q=True, num_experiments=20, num_experiments_pi=100)\n",
    "    results_max.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_results(results_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
