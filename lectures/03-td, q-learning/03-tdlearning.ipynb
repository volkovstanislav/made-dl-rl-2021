{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS, nA = env.env.nS, env.env.nA\n",
    "final_states = np.where([ len(env.env.P[x][0]) == 1 and env.env.P[x][0][0][3] == True for x in env.env.P.keys() ])[0]\n",
    "\n",
    "def get_random_V(env):\n",
    "    V = np.random.random(nS)\n",
    "    V[final_states] = 0.0\n",
    "    return V\n",
    "\n",
    "def get_random_Q(env):\n",
    "    Q = np.random.random(size=(nS, nA))\n",
    "    Q[final_states, :] = 0.0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_by_policy(env, pi, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][pi[cur_state]] ]) \\\n",
    "        for cur_state in range(nS) ])\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_policy_by_V(env, V, gamma=1.0):\n",
    "    return np.argmax( np.array([[ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[s][a] ]) \\\n",
    "        for a in range(nA) ] for s in range(nS)]), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_and_pi(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    pi = np.random.randint(nA, size=nS)\n",
    "\n",
    "    while True:\n",
    "        V = compute_V_by_policy(env, pi, gamma)\n",
    "        new_pi = compute_policy_by_V(env, V, gamma)\n",
    "        if np.array_equal(pi, new_pi):\n",
    "            break\n",
    "        pi = new_pi\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "num_experiments = 200\n",
    "num_steps, total_reward = [], []\n",
    "\n",
    "V, pi = compute_V_and_pi(env)\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    env.reset()\n",
    "    total_reward.append(0)\n",
    "    for step in range(1000):\n",
    "        observation, reward, done, info = env.step(pi[env.env.s])\n",
    "        total_reward[-1] += reward\n",
    "        if done:\n",
    "            num_steps.append(step+1)\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "print(\"\\nСредняя награда: %.6f\\nСреднее число шагов: %.6f\" % (np.mean(total_reward), np.mean(num_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiments_pi(env, pi, num_experiments=1000):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        env.reset()\n",
    "        num_steps.append(0)\n",
    "        total_reward.append(0)\n",
    "        for _ in range(1000):\n",
    "            observation, reward, done, info = env.step(pi[env.env.s])\n",
    "            total_reward[-1] += reward\n",
    "            num_steps[-1] += 1\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return np.mean(total_reward), np.mean(num_steps)\n",
    "\n",
    "def conduct_experiments(env, gamma=1.0, num_experiments=100, num_experiments_pi=10):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        V, pi = compute_V_and_pi(env, gamma=gamma)\n",
    "        cur_steps, cur_reward = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(cur_steps)\n",
    "        total_reward.append(cur_reward)\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "results = []\n",
    "for gamma in np.linspace(0.5, 0.95, 10):\n",
    "    mean_reward, mean_steps = conduct_experiments(env, gamma, num_experiments=100, num_experiments_pi=10)\n",
    "    results.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    gammas, rewards, numsteps = [x[0] for x in results], [x[1] for x in results], [x[2] for x in results]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.grid(None)\n",
    "\n",
    "    line1 = ax.plot(gammas, rewards, label=\"Средние награды\", color=\"C0\")\n",
    "    line2 = ax2.plot(gammas, numsteps, label=\"Среднее число шагов\", color=\"C1\")\n",
    "\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc=2)\n",
    "    ax.set_xlim((0.5, 0.95))\n",
    "    # ax.set_ylim((0.1, 0.8))\n",
    "    # ax2.set_ylim((10, 45))\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_max(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "        new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_Q_max(env, gamma=1.0):\n",
    "    Q = get_random_Q(env)\n",
    "    while True:\n",
    "        new_Q = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * np.max(Q[x[1], :]) ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "#         new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((Q - new_Q) ** 2) < 0.001:\n",
    "            break\n",
    "        Q = new_Q\n",
    "    return Q\n",
    "\n",
    "def compute_policy_by_Q(env, Q, gamma=1.0):\n",
    "    return np.argmax( Q, axis=1 )\n",
    "\n",
    "def conduct_experiments_max(env, gamma, use_Q=False, num_experiments=100, num_experiments_pi=200):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        if use_Q:\n",
    "            Q = compute_Q_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_Q(env, Q)\n",
    "        else:\n",
    "            V = compute_V_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_V(env, V)\n",
    "        result = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(result[0])\n",
    "        total_reward.append(result[1])\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "V = compute_V_max(env)\n",
    "pi = compute_policy_by_V(env, V, gamma=0.99)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "Q = compute_Q_max(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "results_max = []\n",
    "for gamma in np.linspace(0.5, 0.95, 10):\n",
    "    mean_reward, mean_steps = conduct_experiments_max(env, gamma, use_Q=True, num_experiments=20, num_experiments_pi=100)\n",
    "    results_max.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_results(results_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit Monte Carlo по состояниям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, pi):\n",
    "    env.reset()\n",
    "    states, rewards = [env.env.s], [0]\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(pi[env.env.s])\n",
    "        states.append(env.env.s)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return states, rewards\n",
    "\n",
    "def update_returns(R, states, rewards, R_all=None):\n",
    "    state_first_visit = [-1 for _ in range(nS)]\n",
    "    for t, state in enumerate(states):\n",
    "        if state_first_visit[state] == -1:\n",
    "            state_first_visit[state] = t\n",
    "    g = 0\n",
    "    if state_first_visit[states[-1]] == len(states)-1:\n",
    "        R[states[-1]].append(g)\n",
    "        if R_all is not None:\n",
    "            R_all[states[t]].append(g)\n",
    "    for t in range(len(states)-2, -1, -1):\n",
    "        g =  g * gamma + rewards[t+1]\n",
    "        if state_first_visit[states[t]] == t:\n",
    "            R[states[t]].append(g)\n",
    "            if R_all is not None:\n",
    "                R_all[states[t]].append(g)\n",
    "    for state in range(nS):\n",
    "        if state_first_visit[state] == -1:\n",
    "            R_all[state].append(np.nan)\n",
    "    return R, R_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 1000\n",
    "gamma = 0.9\n",
    "\n",
    "Q = compute_Q_max(env, gamma)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "V = get_random_V(env)\n",
    "R, R_all = [ [] for _ in range(nS) ], [ [] for _ in range(nS) ]\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, rewards = run_episode(env, pi)\n",
    "    R, R_all = update_returns(R, states, rewards, R_all=R_all)\n",
    "\n",
    "for state in range(nS):\n",
    "    if len(R[state]) > 0:\n",
    "        V[state] = np.mean(R[state])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = [ np.true_divide( np.nancumsum(R_all[s]), np.arange(1, len(R_all[s])+1) ) for s in range(nS) ]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "for i,s in enumerate([0, 1, 9, 14]):\n",
    "    ax.plot(np.arange(1, len(R_all[s])+1), mean_returns[s], label=\"Состояние %d\" % s, color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"center right\")\n",
    "ax.set_xlim((1, 1000))\n",
    "# ax.set_ylim((0.1, 0.8))\n",
    "# ax2.set_ylim((10, 45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_actions(env, pi, eps=0.0):\n",
    "    env.reset()\n",
    "    next_action = pi[env.env.s] if np.random.rand() > eps else np.random.randint(nA)\n",
    "    states, actions, rewards = [env.env.s], [next_action], [0]\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(next_action)\n",
    "        states.append(env.env.s)\n",
    "        next_action = pi[env.env.s] if np.random.rand() > eps else np.random.randint(nA)\n",
    "        actions.append(next_action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def update_returns_actions(R, states, actions, rewards, gamma=1.0, R_all=None):\n",
    "    state_first_visit = [-1 for _ in range(nS)]\n",
    "    for t, state in enumerate(states):\n",
    "        if state_first_visit[state] == -1:\n",
    "            state_first_visit[state] = t\n",
    "    g = 0\n",
    "    if R_all is not None:\n",
    "        _ = [R_all[s][a].append(np.nan) for a in range(nA) for s in range(nS)]\n",
    "    if state_first_visit[states[-1]] == len(states)-1:\n",
    "        R[states[-1]][actions[-1]].append(g)\n",
    "        if R_all is not None:\n",
    "            R_all[states[t]][actions[t]][-1] = g\n",
    "    for t in range(len(states)-2, -1, -1):\n",
    "        g =  g * gamma + rewards[t+1]\n",
    "        if state_first_visit[states[t]] == t:\n",
    "            R[states[t]][actions[t]].append(g)\n",
    "            if R_all is not None:\n",
    "                R_all[states[t]][actions[t]][-1] = g\n",
    "    return R, R_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 20000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "R, R_all = [ [ [] for _ in range(nA) ] for _ in range(nS) ], [ [ [] for _ in range(nA) ] for _ in range(nS) ]\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, actions, rewards = run_episode_actions(env, pi, eps=0.1)\n",
    "    R, R_all = update_returns_actions(R, states, actions, rewards, gamma=gamma, R_all=R_all)\n",
    "\n",
    "Q = [ [np.mean(R[s][a]) if len(R[s][a]) > 0 else -1 for a in range(nA)] for s in range(nS)]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"Оптимальная функция Q:\\n%s\\n\\nПолученная нами функция Q:\\n%s\" % (Q_max, np.array(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_returns(R_all):\n",
    "    mean_returns = [[[] for _ in range(nA)] for _ in range(nS)]\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            cur_denominator, cur_value = 0., 0.\n",
    "            for r in R_all[s][a]:\n",
    "                if not np.isnan(r):\n",
    "                    cur_denominator += 1.\n",
    "                    cur_value += r\n",
    "                mean_returns[s][a].append(cur_value / cur_denominator if cur_denominator > 0 else 0)\n",
    "    return mean_returns\n",
    "                \n",
    "mean_returns = get_mean_returns(R_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ss = 14\n",
    "for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "# for i,s_a in enumerate([(0,0), (4,0), (14,1), (13,2)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, len(R_all[s][a])+1), mean_returns[s][a], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, 2000))\n",
    "# ax.set_ylim((0.1, 0.8))\n",
    "# ax2.set_ylim((10, 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo control с on-policy исследованием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_by_R(R, Q_default=None):\n",
    "    Q = Q_default if Q_default is not None else np.zeros((nS, nA))\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            if len(R[s][a]) > 0:\n",
    "                Q[s][a] = np.mean(R[s][a])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "init_episodes, total_episodes = 100, 10000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi_max = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "R, R_all = [ [ [] for _ in range(nA) ] for _ in range(nS) ], [ [ [] for _ in range(nA) ] for _ in range(nS) ]\n",
    "\n",
    "Q = get_random_Q(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "for _ in range(init_episodes):\n",
    "    states, actions, rewards = run_episode_actions(env, pi, eps=0.1)\n",
    "    R, R_all = update_returns_actions(R, states, actions, rewards, gamma=gamma, R_all=R_all)\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, actions, rewards = run_episode_actions(env, pi, eps=0.1)\n",
    "    R, R_all = update_returns_actions(R, states, actions, rewards, gamma=gamma, R_all=R_all)\n",
    "    Q = get_Q_by_R(R, Q_default=Q)\n",
    "    pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Лучшая стратегия:\\t%s\" % pi_max)\n",
    "print(\"Текущая стратегия:\\t%s\" % pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = get_mean_returns(R_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ss = 0\n",
    "for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, len(R_all[s][a])+1), mean_returns[s][a], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlim((1, 10000))\n",
    "ax.set_xlabel(\"Эпизоды\")\n",
    "ax.set_ylabel(\"Текущая оценка\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo control с off-policy исследованием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lists(s, a, g, w, Q, C, Q_all):\n",
    "    C[s][a] = C[s][a] + w\n",
    "    Q[s][a] = Q[s][a] + (g - Q[s][a]) * w / C[s][a]\n",
    "    if Q_all is not None:\n",
    "        Q_all[s][a][-1] = g\n",
    "\n",
    "\n",
    "def update_returns_actions_offpolicy_MC(Q, C, pi, states, actions, rewards, epsilon=0.1, gamma=1.0, Q_all=None):\n",
    "    g, w, prob_best_action = 0., 1., 1 - (nA-1) * epsilon / nA\n",
    "    if Q_all is not None:\n",
    "        _ = [Q_all[s][a].append(np.nan) for a in range(nA) for s in range(nS)]\n",
    "\n",
    "    # Обновляем функции для последнего хода\n",
    "    update_lists(states[-1], actions[-1], g, w, Q, C, Q_all)\n",
    "    \n",
    "    for t in range(len(states)-2, -1, -1):\n",
    "        # Если действие не соответствует лучшей стратегии, дальше не надо смотреть\n",
    "        if actions[t+1] != pi[states[t+1]]:\n",
    "            break\n",
    "            \n",
    "        # Обновляем веса и return\n",
    "        w = w / ( prob_best_action )\n",
    "        g =  g * gamma + rewards[t+1]\n",
    "        \n",
    "        # Обновляем функции\n",
    "        update_lists(states[t], actions[t], g, w, Q, C, Q_all)\n",
    "    return Q, C, Q_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 250000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi_max = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "Q_all = [ [ [] for _ in range(nA) ] for _ in range(nS) ]\n",
    "\n",
    "Q = get_random_Q(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "C = np.zeros((nS, nA), dtype=float)\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, actions, rewards = run_episode_actions(env, pi, eps=0.1)\n",
    "    Q, C, Q_all = update_returns_actions_offpolicy_MC(Q, C, pi, states, actions, rewards, gamma=gamma, Q_all=Q_all)\n",
    "    pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"Оптимальная функция Q:\\n%s\\n\\nПолученная нами функция Q:\\n%s\" % (Q_max, np.array(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Лучшая стратегия:\\n%s\" % np.reshape(pi_max, (4, 4)))\n",
    "print(\"Текущая стратегия:\\n%s\" % np.reshape(pi, (4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = get_mean_returns(Q_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ss = 14\n",
    "for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, len(Q_all[s][a])+1), mean_returns[s][a], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, 50000))\n",
    "ax.set_xlabel(\"Эпизоды\")\n",
    "ax.set_ylabel(\"Текущая оценка\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-обучение: on-policy (Sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa_learning_episode(env, pi, Q, alpha=0.05, epsilon=0.0, gamma=0.9):\n",
    "    env.reset()\n",
    "    s, a = env.env.s, pi[env.env.s] if np.random.rand() > epsilon else np.random.randint(nA)\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(a)\n",
    "        s_prime, a_prime = env.env.s, pi[env.env.s] if np.random.rand() > epsilon else np.random.randint(nA)\n",
    "        Q[s][a] = Q[s][a] + alpha * (reward + gamma * Q[s_prime][a_prime] - Q[s][a])\n",
    "        s, a = s_prime, a_prime\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 20000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi_max = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "Q_hist_Sarsa = [ ]\n",
    "\n",
    "Q = get_random_Q(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "for n in range(1, total_episodes+1):\n",
    "    Sarsa_learning_episode(env, pi, Q, alpha=0.1, epsilon= 1. / (np.log(n)+1) , gamma=gamma)\n",
    "    pi = compute_policy_by_Q(env, Q)\n",
    "    Q_hist_Sarsa.append(np.copy(Q))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Лучшая стратегия:\\n%s\" % np.reshape(pi_max, (4, 4)))\n",
    "print(\"Текущая стратегия:\\n%s\" % np.reshape(pi, (4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"Оптимальная функция Q:\\n%s\\n\\nПолученная нами функция Q:\\n%s\" % (Q_max, np.array(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ss = 9\n",
    "for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, total_episodes+1), [q[s][a] for q in Q_hist_Sarsa], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, total_episodes))\n",
    "ax.set_xlabel(\"Эпизоды\")\n",
    "ax.set_ylabel(\"Текущая оценка\")\n",
    "ax.set_ylim((0, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем, как хорошо работают стратегии и как быстро учатся\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "num_experiments, num_experiments_pi = 10, 50\n",
    "\n",
    "total_episodes = 1000\n",
    "gamma = 0.9\n",
    "\n",
    "results_sarsa = []\n",
    "\n",
    "Q_arr = [ get_random_Q(env) for _ in range(num_experiments) ]\n",
    "pi_arr = [ compute_policy_by_Q(env, Q) for Q in Q_arr ]\n",
    "\n",
    "for n in range(1, total_episodes+1):\n",
    "    _ = [ Sarsa_learning_episode(env, pi_arr[i], Q_arr[i], alpha=0.1, epsilon= 1. / (np.log(n)+1) , gamma=gamma) for i in range(num_experiments) ]\n",
    "    pi_arr = [ compute_policy_by_Q(env, Q) for Q in Q_arr ]\n",
    "    result = [ conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi) for pi in pi_arr ]\n",
    "    results_sarsa.append([ [x[0], x[1]] for x in result ])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax2 = ax.twinx()\n",
    "ax2.grid(None)\n",
    "\n",
    "avg_returns = np.mean( np.array([ [x[0] for x in res] for res in results_sarsa]), axis=1)\n",
    "avg_steps = np.mean( np.array([ [x[1] for x in res] for res in results_sarsa]), axis=1)\n",
    "\n",
    "line1 = ax.plot(np.arange(1, total_episodes+1), avg_returns, label=\"Средний результат стратегии в Sarsa\", color=\"C0\")\n",
    "line2 = ax2.plot(np.arange(1, total_episodes+1), avg_steps, label=\"Среднее число шагов в Sarsa\", color=\"C1\")\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax.legend(lines, labels, loc=0)\n",
    "# ax.set_xlim((0.5, 1.0))\n",
    "# ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, total_episodes))\n",
    "ax.set_xlabel(\"Эпизоды\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-обучение: off-policy (Q-обучение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_episode(env, pi, Q, alpha=0.05, epsilon=0.0, gamma=0.9):\n",
    "    env.reset()\n",
    "    s, a = env.env.s, pi[env.env.s] if np.random.rand() > epsilon else np.random.randint(nA)\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(a)\n",
    "        s_prime, a_prime = env.env.s, pi[env.env.s] if np.random.rand() > epsilon else np.random.randint(nA)\n",
    "        Q[s][a] = Q[s][a] + alpha * (reward + gamma * np.max( Q[s_prime] ) - Q[s][a])\n",
    "        s, a = s_prime, a_prime\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 15000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi_max = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "Q_hist_Qlearn = [ ]\n",
    "\n",
    "Q = get_random_Q(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "for n in range(total_episodes):\n",
    "    Q_learning_episode(env, pi, Q, alpha=0.1, epsilon= 0.1, gamma=gamma)\n",
    "    pi = compute_policy_by_Q(env, Q)\n",
    "    Q_hist_Qlearn.append(np.copy(Q))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Лучшая стратегия:\\n%s\" % np.reshape(pi_max, (4, 4)))\n",
    "print(\"Текущая стратегия:\\n%s\" % np.reshape(pi, (4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"Оптимальная функция Q:\\n%s\\n\\nПолученная нами функция Q:\\n%s\" % (Q_max, np.array(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ss = 14\n",
    "for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, total_episodes+1), [q[s][a] for q in Q_hist_Qlearn], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, total_episodes))\n",
    "ax.set_xlabel(\"Эпизоды\")\n",
    "ax.set_ylabel(\"Текущая оценка\")\n",
    "# ax.set_ylim((0, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем, как хорошо работают стратегии и как быстро учатся\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "num_experiments, num_experiments_pi = 10, 50\n",
    "\n",
    "total_episodes = 1000\n",
    "gamma = 0.9\n",
    "\n",
    "results_Qlearn = []\n",
    "\n",
    "Q_arr = [ get_random_Q(env) for _ in range(num_experiments) ]\n",
    "pi_arr = [ compute_policy_by_Q(env, Q) for Q in Q_arr ]\n",
    "\n",
    "for n in range(1, total_episodes+1):\n",
    "    _ = [ Q_learning_episode(env, pi_arr[i], Q_arr[i], alpha=0.1, epsilon= .1 , gamma=gamma) for i in range(num_experiments) ]\n",
    "    pi_arr = [ compute_policy_by_Q(env, Q) for Q in Q_arr ]\n",
    "    result = [ conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi) for pi in pi_arr ]\n",
    "    results_Qlearn.append([ [x[0], x[1]] for x in result ])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax2 = ax.twinx()\n",
    "ax2.grid(None)\n",
    "\n",
    "avg_returns_Q = np.mean( np.array([ [x[0] for x in res] for res in results_Qlearn]), axis=1)\n",
    "avg_steps_Q = np.mean( np.array([ [x[1] for x in res] for res in results_Qlearn]), axis=1)\n",
    "\n",
    "line1 = ax.plot(np.arange(1, total_episodes+1), avg_returns, label=\"Средний результат стратегии в Sarsa\", color=\"C0\", alpha=0.4)\n",
    "line2 = ax2.plot(np.arange(1, total_episodes+1), avg_steps, label=\"Среднее число шагов в Sarsa\", color=\"C1\", alpha=0.4)\n",
    "line3 = ax.plot(np.arange(1, total_episodes+1), avg_returns_Q, label=\"Средний результат стратегии в Q-обучении\", color=\"C2\")\n",
    "line4 = ax2.plot(np.arange(1, total_episodes+1), avg_steps_Q, label=\"Среднее число шагов в Q-обучении\", color=\"C3\")\n",
    "\n",
    "lines = line1 + line3 + line2 + line4\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax.legend(lines, labels, loc=\"upper left\")\n",
    "# ax.set_xlim((0.5, 1.0))\n",
    "ax.set_xlim((1, total_episodes))\n",
    "ax.set_xlabel(\"Эпизоды\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
